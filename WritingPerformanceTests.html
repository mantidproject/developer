<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Writing Performance Tests</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="System Tests" href="SystemTests.html" />
    <link rel="prev" title="Reviewing a Pull Request" href="ReviewingAPullRequest.html" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59110517-1', 'auto');
  ga('send', 'pageview');

</script>


  </head><body>





  <div id="navbar" class="navbar navbar-default ">
    <div class="container">
      <div class="navbar-header">
        
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="http://www.mantidproject.org"><img src="_static/Mantid_Logo_Transparent.png">
           </a>
        <span class="navbar-text navbar-version pull-left"><b>master</b></span>
      </div>

      
        <div class="collapse navbar-collapse nav-collapse">
      
          <ul class="nav navbar-nav">
            <li class="divider-vertical"></li>
            
                <li><a href="index.html">Home</a></li>
                <li><a href="http://download.mantidproject.org">Download</a></li>
                <li><a href="http://www.mantidproject.org">Wiki</a></li>
                <li><a href="http://docs.mantidproject.org">User Documentation</a></li>
                <li><a href="http://www.mantidproject.org/Contact">Contact Us</a></li>
            
            
              
              
            
            
            
            
          </ul>
              
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
            </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section id="writing-performance-tests">
<span id="writingperformancetests"></span><h1>Writing Performance Tests<a class="headerlink" href="#writing-performance-tests" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id1">Overview</a></p></li>
<li><p><a class="reference internal" href="#how-to-write-c-performance-tests" id="id2">How to Write C++ Performance Tests</a></p></li>
<li><p><a class="reference internal" href="#running-performance-tests" id="id3">Running Performance Tests</a></p>
<ul>
<li><p><a class="reference internal" href="#running-tests-without-ctest" id="id4">Running tests without ctest</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#best-practice-advice" id="id5">Best Practice Advice</a></p></li>
<li><p><a class="reference internal" href="#avoiding-compiler-optimizations" id="id6">Avoiding Compiler Optimizations</a></p></li>
<li><p><a class="reference internal" href="#jobs-that-monitor-performance" id="id7">Jobs that monitor performance</a></p></li>
</ul>
</nav>
<section id="overview">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>The point of Performance Tests is to track the performance of a piece of
code through time, so that it will be easy to pinpoint any change that
reduced the performance of a particular bit of code. Performance Tests
will be built and run by a build job, that will track the history of
timing vs revision. The Jenkins job that runs the performance tests
sends out an email when the performance of a test is lowered by more
than a threshold percentage (currently 25%), relative to the average of
a few previous tests.</p>
</section>
<section id="how-to-write-c-performance-tests">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">How to Write C++ Performance Tests</a><a class="headerlink" href="#how-to-write-c-performance-tests" title="Permalink to this heading">¶</a></h2>
<p>An ideal performance test is neither too fast, nor too slow. The
precision of timing will be insufficient if the test runs in much less
than a second; tests that run for several minutes should also be
avoided.</p>
<p>C++ performance tests are written in the same way as unit tests, and in
the same file as the unit tests for a particular class, except that you
will add <em>Performance</em> to the end of the name of the test suite. For
example, in MyAlgorithmTest.h:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyAlgorithmTest</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">CxxTest</span><span class="o">::</span><span class="n">TestSuite</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">   </span><span class="c1">// Put in your usual, quick unit tests here</span>
<span class="p">};</span><span class="w"></span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyAlgorithmTestPerformance</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">CxxTest</span><span class="o">::</span><span class="n">TestSuite</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="k">public</span><span class="o">:</span><span class="w"></span>
<span class="w">   </span><span class="n">MatrixWorkspace_sptr</span><span class="w"> </span><span class="n">WS</span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="kt">int</span><span class="w"> </span><span class="n">numpixels</span><span class="p">;</span><span class="w"></span>

<span class="w">   </span><span class="kt">void</span><span class="w"> </span><span class="nf">setUp</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="c1">// Put in any code needed to set up your test,</span>
<span class="w">      </span><span class="c1">// but that should NOT be counted in the execution time.</span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="kt">void</span><span class="w"> </span><span class="nf">tearDown</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="c1">// Clean-up code, also NOT counted in execution time.</span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="kt">void</span><span class="w"> </span><span class="nf">test_slow_performance</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="c1">// Put in a unit test that will be slow.</span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>
</pre></div>
</div>
<p>Only the presence/absence of the word Performance is used to determine
if it is a Performance test. As with unit tests, your suite name has to
match the file name.</p>
<p>You may include ASSERT’s in the same way as a unit test to check that
results are meaningful, but that is unnecessary since these tests are
for performance, not function. Avoid adding so many assert’s that it
would slow down your test.</p>
</section>
<section id="running-performance-tests">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Running Performance Tests</a><a class="headerlink" href="#running-performance-tests" title="Permalink to this heading">¶</a></h2>
<p>Performance tests targets are not added by default when running
cmake/make. To add them as ctest targets, enable them with the flag:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>cmake -DCXXTEST_ADD_PERFORMANCE<span class="o">=</span>TRUE
</pre></div>
</div>
<p>After re-building, you can then run performance tests with the command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>ctest <span class="o">[</span>-C Release<span class="p">|</span>Debug<span class="o">]</span> -R Performance
</pre></div>
</div>
<p>And run regular unit tests, excluding the slow performance ones, with:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>ctest <span class="o">[</span>-C Release<span class="p">|</span>Debug<span class="o">]</span> -E Performance
</pre></div>
</div>
<p>where the -C option is required for multi-configuration builds like
Visual Studio &amp; XCode.</p>
<p>The resulting .XML files will contain the detailed timing (of just the
test portion without setUp/tearDown time).</p>
<p>Note that newly added performance tests will not be registered with
ctest until cmake has been re-run.</p>
<section id="running-tests-without-ctest">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Running tests without ctest</a><a class="headerlink" href="#running-tests-without-ctest" title="Permalink to this heading">¶</a></h3>
<p>The tests are still built into every test executable, even if you have
not set the flag. For example,</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>AlgorithmsTest --help-tests
</pre></div>
</div>
<p>will list all the available tests. If you run</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>AlgorithmsTest
</pre></div>
</div>
<p>alone, it will SKIP the Performance Tests. You have to give the name of
the specific test suite you want to run, e.g,</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>AlgorithmsTest MyAlgorithmPerformanceTest
</pre></div>
</div>
</section>
</section>
<section id="best-practice-advice">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Best Practice Advice</a><a class="headerlink" href="#best-practice-advice" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Performance tests are not System Tests. They should test the code at
the same granularity as the unit test suite.</p></li>
<li><p>Performance tests are not Unit Tests. There is no need to perform
lots of assertions on the test results.</p></li>
<li><p>Performance tests should perform enough work such that statistically
significant performance differences can be measured.</p></li>
<li><p>The performance tests are executed often, so ideally they should
typically take 0.2 - 2 seconds to run.</p></li>
<li><p>Always perform test set-up outside of the test method. That way your
timings will only relate to the target code you wish to measure.</p></li>
</ul>
</section>
<section id="avoiding-compiler-optimizations">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Avoiding Compiler Optimizations</a><a class="headerlink" href="#avoiding-compiler-optimizations" title="Permalink to this heading">¶</a></h2>
<p>A common issue with performance tests is when you want to compute some
value over a large number of iterations but ultimately you don’t want
to actually use it in the end. We have included some handy utility functions
to avoid the compiler optimizing unused variables away. For more information
this conference talk is quite enlightening: <a class="reference external" href="https://youtu.be/nXaxk27zwlk?t=2441">https://youtu.be/nXaxk27zwlk?t=2441</a>.</p>
<p>For example, we attempt to time the sum of a value after many calls to an operation:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;CxxTest/BenchmarkUtil.h&gt;</span><span class="cp"></span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyTestPerformance</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">CxxTest</span><span class="o">::</span><span class="n">TestSuite</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="k">public</span><span class="o">:</span><span class="w"></span>

<span class="w">   </span><span class="kt">void</span><span class="w"> </span><span class="n">test_slow_performance</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">     </span><span class="kt">double</span><span class="w"> </span><span class="nf">value</span><span class="p">(</span><span class="mf">0.0</span><span class="p">);</span><span class="w"></span>
<span class="w">     </span><span class="k">for</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1000000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">       </span><span class="n">value</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">my_op</span><span class="p">();</span><span class="w"></span>
<span class="w">       </span><span class="n">CxxTest</span><span class="o">::</span><span class="n">doNotOptimize</span><span class="p">(</span><span class="o">&amp;</span><span class="n">value</span><span class="p">);</span><span class="w"></span>
<span class="w">     </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>
</pre></div>
</div>
<p>Without the <code class="docutils literal notranslate"><span class="pre">CxxTest::doNotOptimize(&amp;value);</span></code> call the compiler will likely spot
that <code class="docutils literal notranslate"><span class="pre">value</span></code> is ultimately unused and the whole function will be optimized away
and the benchmark will become useless.</p>
</section>
<section id="jobs-that-monitor-performance">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Jobs that monitor performance</a><a class="headerlink" href="#jobs-that-monitor-performance" title="Permalink to this heading">¶</a></h2>
<p>There is a job in Jenkins (our continuous integration system) that runs
the performance test suite and generates output that enables us to
easily monitor timings. The job runs a set of <a class="reference external" href="http://builds.mantidproject.org/job/main_performancetests2/">performance tests on the
main branch of
Mantid</a>.
This job runs on a machine at the ESS, everytime that changes are merged
into the Mantid master branch, and stores the timing information in a
database, also generating HTML output via a <a class="reference external" href="https://github.com/mantidproject/mantid/tree/main/Testing/PerformanceTests">set of python
scripts</a>.</p>
<p>The timing output of these jobs are typically monitored manually on a
weekly basis to pick up any notable performance degradation. Although
automatic checking is available within the python scripts, the level of
instability in the timings meant that it always produced way too many
false positives to be useful.</p>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
      <ul class="nav navbar-nav" style=" float: right;">
      
      
          
            
  <li>
    <a href="ReviewingAPullRequest.html" title="Previous Chapter: Reviewing a Pull Request"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Reviewing a P...</span>
    </a>
  </li>
  <li>
    <a href="SystemTests.html" title="Next Chapter: System Tests"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">System Tests &raquo;</span>
    </a>
  </li>
          
       
          <li><a href="#">Back to top</a></li>
       </ul>
    <p>
    </p>
  </div>
</footer>
  </body>
</html>